{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required modules.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        #encoder\n",
    "        self.e1 = nn.Linear(input_size,256)\n",
    "        self.e2 = nn.Linear(256,128)\n",
    "        \n",
    "        #Latent View\n",
    "        self.lv = nn.Linear(128,16)\n",
    "        \n",
    "        #Decoder\n",
    "        self.d1 = nn.Linear(16,64)\n",
    "        self.d2 = nn.Linear(64,128)\n",
    "        \n",
    "        self.output_layer = nn.Linear(128,output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.e1(x))\n",
    "        x = F.relu(self.e2(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.lv(x))\n",
    "        \n",
    "        x = F.relu(self.d1(x))\n",
    "        x = F.relu(self.d2(x))\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
